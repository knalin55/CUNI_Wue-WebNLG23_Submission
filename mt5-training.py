# -*- coding: utf-8 -*-
"""t5_base_C2T_big.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v7Az53Nqu2wDRr0MZqSJD1CWKJ0JW0J4
"""
'''
!pip3 install torch
!pip3 install torchvision
!pip3 install sentencepiece
!pip3 install transformers
!pip3 install datasets
!pip install rouge-score
!pip3 install wandb
!pip install accelerate

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')
'''
import datetime
import os
import time
import sys

import numpy as np
import random
import pandas as pd
import nltk
import numpy as np
import json
import wandb
import torch
from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler
torch.manual_seed(50)

from datasets.dataset_dict import DatasetDict
from datasets import load_metric, load_dataset 
from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
import argparse


parser = argparse.ArgumentParser()
parser.add_argument("--languages", default="ga", type=str, help="[ga/mt/br/cy/ru/")
parser.add_argument("--model", default="google/mt5-large", type=str, help="Seq to Seq Model")


args = parser.parse_args()
wandb.login()

nltk.download('punkt')

#EDIT PATHS
base_path = '/work/sobaidul/mt5-large-webnlg'
data_path = f'{base_path}/data_splits'
lang = args.languages
print('Running for language: ' + lang + '\n')


train_data = load_dataset('csv', data_files=f'{data_path}/{lang}_{lang}_train.tsv', sep='\t')
val_data = load_dataset('csv', data_files=f'{data_path}/{lang}_{lang}_dev.tsv', sep='\t')
dataset = DatasetDict({
    'train': train_data['train'],
    'valid': val_data['train']})
print(dataset)

#dataset = dataset.remove_columns(['eid','otriples_sets','category','lang','comment'])

metric = load_metric("sacrebleu")

model_checkpoint = 'google/mt5-large'
device = "cuda:0" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device_map="auto")
model =  AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

#EDIT PREFIX
prefix = f"triple to text for {lang}: "

max_input_length = 1024
max_target_length = 512

def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["triple"] ]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["text"], max_length=max_target_length, truncation=True,  padding='max_length')

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result


tokenized_datasets = dataset.map(preprocess_function, batched=True)
print(tokenized_datasets)


batch_size = 1
model_name = model_checkpoint.split("/")[-1]
args = Seq2SeqTrainingArguments(
    output_dir = f"mt5large-{lang}-version-2",
    learning_rate=2e-05,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=12,
    predict_with_generate=True,
    logging_dir="./logs",
    report_to="wandb",
    gradient_accumulation_steps=8,
    evaluation_strategy = "epoch",
    save_strategy= "epoch"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding = True, return_tensors="pt")

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.save_model(f'{base_path}/mT5-{lang}-large')

