# -*- coding: utf-8 -*-
"""t5base_inference_clean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kg0t5i1mWQafU7EHR-lbXGPOM_VtsIfm
"""
import datetime
import os
import time
import sys
import re
import numpy as np
import random
import pandas as pd
import nltk
import numpy as np
import json

import torch
from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler
torch.manual_seed(49)

from datasets.dataset_dict import DatasetDict
from datasets import load_metric, load_dataset 
from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import statistics


languages = ['ru', 'mt', 'cy','ga','br']




def get_triple_by_subj(triplet_list):

  subject_dict = {}
  final_output = list()
  
  for triplet in triplet_list:
      # Split each triplet into subject, predicate and object
      subject, predicate, obj = triplet.split(' | ')
      # Strip whitespaces from the subject
      subject = subject.strip()
      # Add subject not in the dictionary
      if subject not in subject_dict:
          subject_dict[subject] = []
      # Add the triplet to the list corresponding to the subject
      subject_dict[subject].append(triplet)

  # Now, if you want the output as a list of lists:
  output = list(subject_dict.values())

  for o in output:
    final_output.append(' $$ '.join(o))

  return final_output


def generate_summaries_beam(tests, lang):
    
    max_input_length = 1024
    max_target_length = 512
    hypo_summary = list()
    counter = 0
    for text in tests:
        if counter % 200 == 0:
            print(counter)
        part_sums = list()
        # --------------------- REPLACE THE SPECIAL CHARACTER THAT YOU USE TO SPLIT THE TRIPLES
        text = text.split('$$')
        text = get_triple_by_subj(text)
        for t in text:
            #============= REPLACE THE PREFIX WITH YOUR PREFIX =======================
            tokens = tokenizer.encode(f'triple to text for {lang}: ' + t, max_length=max_input_length,  truncation=True, padding='max_length', return_tensors='pt').to('cuda')
            generated = model.generate(tokens, num_beams=4, max_length = 256)
            tgt_text = tokenizer.decode(generated[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
            summary = str(tgt_text).strip('[]""')
            part_sums.append(summary.replace('\n', ''))
        
        hypo_summary.append(' '.join(part_sums))
        counter = counter + 1

    return hypo_summary

for l in languages:

    print(f'Running for language {l}')
    #------- REPLACE THE MODEL CHECKPOINT PATH-------------
    model_checkpoint = f'mT5-{l}-large'
    print('Loading Tokenizer...\n')
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained('google/mt5-large')
    
    print('Loading Model...\n')
    model =   AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)
    #------- REPLACE THE TEST FILE PATH-------------
    test = pd.read_csv(f'data_splits/{l}_{l}_test.tsv', sep='\t')

    test_data = list(test['triple'])
    test_summary = list(test['text'])

    '''
    ref_file = open(f'references_{l}_{l}.txt', 'w')
    for k in test_summary:
        ref_file.write(k + '\n')
    ref_file.close()

    data_file = open(f'data_{l}_{l}.txt', 'w')
    for k in test_data:
        data_file.write(k.replace('\n','') + '\n')
    data_file.close()
    '''


    print(f'Running beam search.\n')
    gen_sum_beam = generate_summaries_beam(test_data,l)

    #------------ EDIT THE NAME OF THE HYPOTHESIS FILE ------------------
    hypo_file = open(f'hypothesis_mt5_beam_{l}_{l}_part_2.txt', 'w')
    for i in gen_sum_beam:
        hypo_file.write(i.replace('\n','') + '\n')
    hypo_file.close()